# -*- coding: utf-8 -*-
"""DiseasePrediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/10qvvZM8dN4uPfvZkb-lnnGXc_XIjpJaT
"""



import pandas as pd
# Step 1: Import libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# For better graph styling
sns.set(style="whitegrid")


df = pd.read_csv('/content/drive/MyDrive/cardio_train.csv',sep=';')
display(df.head())

df.shape
df.info()

# Convert age from days to years and round it
df['age'] = (df['age'] / 365).astype(int)

df['age'].describe()

# Plot distributions of key numeric columns
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(16, 10))

columns = ['age', 'height', 'weight', 'ap_hi', 'ap_lo']

for i, col in enumerate(columns):
    plt.subplot(2, 3, i+1)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
    plt.tight_layout()

print("Systolic BP (ap_hi) min/max:", df['ap_hi'].min(), df['ap_hi'].max())
print("Diastolic BP (ap_lo) min/max:", df['ap_lo'].min(), df['ap_lo'].max())

# Show extreme values
df[df['ap_hi'] > 250].head(10)

# Remove negative and overly high values
df = df[(df['ap_hi'] >= 90) & (df['ap_hi'] <= 250)]
df = df[(df['ap_lo'] >= 60) & (df['ap_lo'] <= 180)]

df = df[df['ap_hi'] > df['ap_lo']]

print("New ap_hi range:", df['ap_hi'].min(), "-", df['ap_hi'].max())
print("New ap_lo range:", df['ap_lo'].min(), "-", df['ap_lo'].max())
print("New dataset shape:", df.shape)

plt.figure(figsize=(12,5))

plt.subplot(1,2,1)
sns.histplot(df['ap_hi'], bins=40, kde=True, color='skyblue')
plt.title('Cleaned Systolic BP (ap_hi)')

plt.subplot(1,2,2)
sns.histplot(df['ap_lo'], bins=40, kde=True, color='salmon')
plt.title('Cleaned Diastolic BP (ap_lo)')

plt.tight_layout()
plt.show()

# Drop unnecessary columns
df = df.drop(columns=['id'])  # 'id' has no predictive power

# Separate features and target
X = df.drop('cardio', axis=1)  # Features
y = df['cardio']               # Target

from sklearn.model_selection import train_test_split

# 80% for training, 20% for testing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

print("Training set:", X_train.shape)
print("Testing set:", X_test.shape)

from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, confusion_matrix

dt = DecisionTreeClassifier(max_depth=5,random_state=42)
dt.fit(X_train, y_train)

# Predict on test set
y_pred = dt.predict(X_test)

# Evaluate the model
print("Classification Report:\n", classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

# Plot feature importances
# üîç Explanation:
# This shows you which features (like age, ap_hi, weight) are most influential in predicting heart disease


import matplotlib.pyplot as plt
import numpy as np

feature_importance = pd.Series(dt.feature_importances_, index=X.columns)
feature_importance = feature_importance.sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=feature_importance, y=feature_importance.index)
plt.title("Feature Importance - Decision Tree")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

from sklearn.ensemble import RandomForestClassifier

# Initialize and train the Random Forest model
rf = RandomForestClassifier(n_estimators=100, random_state=42)
rf.fit(X_train, y_train)
y_pred_rf = rf.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix

# Predict on the test set
y_pred_rf = rf.predict(X_test)

# Classification report
print("Random Forest Classification Report:\n", classification_report(y_test, y_pred_rf))

# Confusion matrix
cm_rf = confusion_matrix(y_test, y_pred_rf)
sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Greens')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Random Forest Confusion Matrix")
plt.show()

# Calculate and sort feature importance
importances_rf = pd.Series(rf.feature_importances_, index=X.columns)
importances_rf = importances_rf.sort_values(ascending=False)

# Plot the top features
plt.figure(figsize=(10,6))
sns.barplot(x=importances_rf, y=importances_rf.index)
plt.title("Feature Importance - Random Forest")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()

!pip install xgboost

from xgboost import XGBClassifier

# Create the XGBoost model
xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)

# Train it on the training data
xgb.fit(X_train, y_train)

# Predict labels on the test set
y_pred_xgb = xgb.predict(X_test)

from sklearn.metrics import classification_report, confusion_matrix

# Classification report
print("XGBoost Classification Report:\n", classification_report(y_test, y_pred_xgb))

cm_xgb = confusion_matrix(y_test, y_pred_xgb)

sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Oranges')
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("XGBoost Confusion Matrix")
plt.show()

# Plot feature importances
importances_xgb = pd.Series(xgb.feature_importances_, index=X.columns)
importances_xgb = importances_xgb.sort_values(ascending=False)

plt.figure(figsize=(10,6))
sns.barplot(x=importances_xgb, y=importances_xgb.index)
plt.title("Feature Importance - XGBoost")
plt.xlabel("Importance Score")
plt.ylabel("Feature")
plt.show()



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Dictionary to hold model results
results = {}

# Decision Tree
results['Decision Tree'] = {
    'Accuracy': accuracy_score(y_test, y_pred),
    'Precision': precision_score(y_test, y_pred),
    'Recall': recall_score(y_test, y_pred),
    'F1 Score': f1_score(y_test, y_pred)
}

# Random Forest
results['Random Forest'] = {
    'Accuracy': accuracy_score(y_test, y_pred_rf),
    'Precision': precision_score(y_test, y_pred_rf),
    'Recall': recall_score(y_test, y_pred_rf),
    'F1 Score': f1_score(y_test, y_pred_rf)
}

# XGBoost
results['XGBoost'] = {
    'Accuracy': accuracy_score(y_test, y_pred_xgb),
    'Precision': precision_score(y_test, y_pred_xgb),
    'Recall': recall_score(y_test, y_pred_xgb),
    'F1 Score': f1_score(y_test, y_pred_xgb)
}

# Convert to pandas DataFrame for pretty display
comparison_df = pd.DataFrame(results).T  # Transpose for better view
comparison_df = comparison_df.round(4)   # Round off to 4 decimal places
comparison_df

from sklearn.metrics import roc_curve, auc

# Get predicted probabilities for each model
y_prob_dt = dt.predict_proba(X_test)[:, 1]     # Decision Tree
y_prob_rf = rf.predict_proba(X_test)[:, 1]     # Random Forest
y_prob_xgb = xgb.predict_proba(X_test)[:, 1]   # XGBoost

# Calculate FPR (x-axis), TPR (y-axis), and AUC
fpr_dt, tpr_dt, _ = roc_curve(y_test, y_prob_dt)
fpr_rf, tpr_rf, _ = roc_curve(y_test, y_prob_rf)
fpr_xgb, tpr_xgb, _ = roc_curve(y_test, y_prob_xgb)

auc_dt = auc(fpr_dt, tpr_dt)
auc_rf = auc(fpr_rf, tpr_rf)
auc_xgb = auc(fpr_xgb, tpr_xgb)

plt.figure(figsize=(8,6))
plt.plot(fpr_dt, tpr_dt, label=f'Decision Tree (AUC = {auc_dt:.2f})', linestyle='--')
plt.plot(fpr_rf, tpr_rf, label=f'Random Forest (AUC = {auc_rf:.2f})', linestyle='-.')
plt.plot(fpr_xgb, tpr_xgb, label=f'XGBoost (AUC = {auc_xgb:.2f})', color='darkorange')

# Diagonal line (random guessing)
plt.plot([0, 1], [0, 1], color='gray', linestyle=':')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid(True)
plt.show()

# from sklearn.model_selection import cross_val_score

# # Decision Tree
# dt_scores = cross_val_score(dt, X, y, cv=5, scoring='accuracy')
# print("Decision Tree CV Accuracy:", dt_scores)
# print("Mean Accuracy:", dt_scores.mean().round(4), "\n")

# # Random Forest
# rf_scores = cross_val_score(rf, X, y, cv=5, scoring='accuracy')
# print("Random Forest CV Accuracy:", rf_scores)
# print("Mean Accuracy:", rf_scores.mean().round(4), "\n")

# # XGBoost
# xgb_scores = cross_val_score(xgb, X, y, cv=5, scoring='accuracy')
# print("XGBoost CV Accuracy:", xgb_scores)
# print("Mean Accuracy:", xgb_scores.mean().round(4))

# Example patient data (values must match model columns)
new_patient = pd.DataFrame({
    'age': [50],           # Age in years
    'gender': [2],         # 1 = Female, 2 = Male
    'height': [172],       # in cm
    'weight': [82],        # in kg
    'ap_hi': [135],        # Systolic BP
    'ap_lo': [85],         # Diastolic BP
    'cholesterol': [2],    # 1 = Normal, 2 = Above normal, 3 = High
    'gluc': [1],           # 1 = Normal, 2 = Above normal, 3 = High
    'smoke': [0],          # 1 = Yes, 0 = No
    'alco': [0],           # 1 = Yes, 0 = No
    'active': [1]          # 1 = Active, 0 = Not active
})

# Predict class (0 or 1)
pred = xgb.predict(new_patient)[0]

# Predict probability
prob = xgb.predict_proba(new_patient)[0][1]

print("üß™ Patient Health Summary:\n")
print(new_patient.T)

print("\nüîç Prediction Result:")
if pred == 1:
    print("‚ö†Ô∏è High Risk of Cardiovascular Disease")
else:
    print("‚úÖ Low Risk (No Disease Detected)")

print(f"üß† Confidence Score: {prob:.2f}")

